{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrozenLake basic Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-30 17:18:51,967] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "learning_rate = 0.85\n",
    "gamma = 0.99\n",
    "num_episodes = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initialize table with all zeros\n",
    "Q = np.zeros([num_states,num_actions])\n",
    "#create lists to contain total rewards and steps per episode\n",
    "reward_list = []\n",
    "count_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    state = env.reset()\n",
    "    All_reward = 0\n",
    "    terminate_state = False\n",
    "    count = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    while count < 99:\n",
    "        count+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        random_factor = np.random.randn(1,num_actions)*(1./(i+1))\n",
    "        action = np.argmax(Q[state,:] + random_factor)\n",
    "        #Get new state and reward from environment\n",
    "        state_prime,reward,terminate_state,_ = env.step(action)\n",
    "        #Update Q-Table with new knowledge\n",
    "        Q[state,action] = Q[state,action] + learning_rate*(reward + gamma*np.max(Q[state_prime,:]) - Q[state,action])\n",
    "        All_reward += reward\n",
    "        state = state_prime\n",
    "        if terminate_state == True:\n",
    "            break\n",
    "    reward_list.append(All_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.7375\n"
     ]
    }
   ],
   "source": [
    "print(\"Score over time: \" +  str(sum(reward_list)/num_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q-Table Values\n",
      "[[  1.58947685e-02   1.59879519e-01   8.19791682e-03   1.18295465e-02]\n",
      " [  8.66036364e-03   1.27163090e-02   9.36279039e-03   9.83602221e-02]\n",
      " [  6.03407056e-03   1.19602607e-02   8.02911510e-03   8.50099408e-02]\n",
      " [  6.18323159e-06   1.99298088e-04   2.61245276e-02   8.55161351e-02]\n",
      " [  1.57407386e-01   1.53826493e-03   6.84397562e-04   1.23110385e-03]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  8.44772351e-05   5.39584388e-05   9.93535126e-02   9.60495417e-06]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  3.60467626e-03   2.23940055e-03   7.04036187e-04   1.69116561e-01]\n",
      " [  5.89759676e-04   4.89531323e-01   2.45251825e-05   0.00000000e+00]\n",
      " [  2.43602780e-04   1.14949771e-04   1.77741058e-02   1.98433375e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  2.94136996e-05   2.86562811e-03   7.60179891e-01   3.55455102e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   9.70774436e-01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Q-Table Values\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
